{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################ Imports ######################################\n",
    "import math\n",
    "\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from datasets import get_dataset_split_names\n",
    "import copy\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "################################ Constants ####################################\n",
    "SENTENCE_TASK2 = (\"I have a house in\").split()  # str -> list\n",
    "FIRST_SENTENCE = (\"START Brad Pitt was born in Oklahoma\").split()  # str -> list\n",
    "SECOND_SENTENCE = (\"START The actor was born in USA\").split()  # str -> list\n",
    "test = (\"START The game began development in\").split()\n",
    "\n",
    "\n",
    "################################ Main functions ###############################\n",
    "\n",
    "def load_data():\n",
    "    df = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # text = df['text'][:10000]\n",
    "    text = df['text']\n",
    "    return text, nlp\n",
    "\n",
    "\n",
    "def procces_data(text, nlp):\n",
    "    words = set()  # contain all distinct lemmas in training corpus\n",
    "    docs = []\n",
    "    for sentence in text:\n",
    "        # sentence = \"START \" + sentence  # I put spaces here be aware\n",
    "        doc = nlp(sentence)\n",
    "        lemma_docs = [token.lemma_ for token in doc if token.is_alpha]\n",
    "        if lemma_docs:\n",
    "            lemma_lower_docs = convert_to_lower_case(lemma_docs)  # convert to array and then to lowercase\n",
    "            docs.append(list(lemma_lower_docs))\n",
    "            for token in lemma_lower_docs:\n",
    "                words.add(token)\n",
    "    return docs, words\n",
    "\n",
    "\n",
    "################################ TASK 1 ###############################\n",
    "\n",
    "def train_unigram_model(docs):\n",
    "    unigram = dict()\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if token in unigram.keys():\n",
    "                unigram[token] += 1\n",
    "            else:\n",
    "                unigram[token] = 1\n",
    "    unigram_model = pd.DataFrame.from_dict(unigram, orient='index')\n",
    "    unigram_model = unigram_model / np.sum(unigram_model.values)\n",
    "    return unigram_model\n",
    "\n",
    "\n",
    "def train_bigram_model(docs, words):\n",
    "    words_arr = np.array([\"START\"] + list(words))\n",
    "    b_gram_model = pd.DataFrame(data=np.zeros((words_arr.shape[0], words_arr.shape[0])), index=words_arr,\n",
    "                                columns=words_arr)\n",
    "    b_gram = dict()  # creating a nested dictionary. to hold biagram\n",
    "    # create b_gram dict and fills it\n",
    "    for doc in docs:\n",
    "        cur_doc = [\"START\"] + doc\n",
    "        for i in range(len(cur_doc) - 1):\n",
    "            word = cur_doc[i]\n",
    "            next_word = cur_doc[i + 1]\n",
    "            if word not in b_gram.keys():\n",
    "                b_gram[word] = dict()\n",
    "            if next_word in b_gram[word].keys():\n",
    "                b_gram[word][next_word] += 1\n",
    "            else:\n",
    "                b_gram[word][next_word] = 1\n",
    "\n",
    "    # fills b_gram_model with propabilites\n",
    "    for word in b_gram.keys():\n",
    "        cur_dict = b_gram[word]\n",
    "        for next_word in cur_dict.keys():\n",
    "            value = cur_dict[next_word]\n",
    "            # b_gram_model[next_word][word] = value #TODO  why its working?\n",
    "            b_gram_model.loc[word][next_word] = value  # TODO  why its working?\n",
    "        b_gram_model.loc[word] /= np.sum(list(cur_dict.values()))\n",
    "\n",
    "    return b_gram_model\n",
    "\n",
    "\n",
    "################################ TASK 2 ###############################\n",
    "def continue_sentence(b_gram_model):\n",
    "    prev_word = SENTENCE_TASK2[-1]\n",
    "    index = np.argmax(b_gram_model.loc[prev_word])\n",
    "    predicted_word = b_gram_model.columns[index]\n",
    "    return predicted_word\n",
    "\n",
    "\n",
    "################################ TASK 3 ###############################\n",
    "def predict_sentence_probabilty_b_gram(sentence, b_gram_model, nlp):\n",
    "    prob = 1\n",
    "    sentence = nlp(str(convert_to_lower_case(sentence)))\n",
    "\n",
    "    sentence = [token.lemma_ for token in sentence if token.is_alpha]\n",
    "    for i in range(len(sentence) - 1):\n",
    "        word = sentence[i]\n",
    "        if i == 0:\n",
    "            word = \"START\"\n",
    "        next_word = sentence[i + 1]\n",
    "        if word not in b_gram_model.keys() or next_word not in b_gram_model.loc[word].keys():\n",
    "            return -math.inf\n",
    "        temp = b_gram_model.loc[word][next_word]  # TODO WHY LIKE THIS\n",
    "        prob *= temp\n",
    "    return prob\n",
    "\n",
    "\n",
    "################################ TASK 4 ###############################\n",
    "\n",
    "# def linear_interpolation(sentence, unigram_model, b_gram_model, nlp):\n",
    "#     bigram_lambda = 2 / 3\n",
    "#     unigram_lambda = 1 / 3\n",
    "#     unigram_prob =  predict_sentence_probabilty_unigram(sentence, unigram_model, nlp)\n",
    "#     # unigram_prob = log_probability(unigram_prob)\n",
    "#     b_gram_prob = predict_sentence_probabilty_b_gram(sentence, b_gram_model, nlp)\n",
    "#     # b_gram_prob = log_probability(b_gram_prob)\n",
    "#     prediction = log_probability(unigram_lambda * unigram_prob+ \\\n",
    "#                  bigram_lambda * b_gram_prob)\n",
    "#     return prediction\n",
    "\n",
    "def linear_interpolation(sentence, unigram_model, b_gram_model, nlp):\n",
    "    bigram_lambda = 2 / 3\n",
    "    unigram_lambda = 1 / 3\n",
    "    prob = 0\n",
    "    sentence = nlp(str(convert_to_lower_case(sentence)))\n",
    "    sentence = [token.lemma_ for token in sentence if token.is_alpha]\n",
    "    for i in range(len(sentence) - 1):\n",
    "        # b_gram\n",
    "        word = sentence[i]\n",
    "        if i == 0:\n",
    "            word = \"START\"\n",
    "        next_word = sentence[i + 1]\n",
    "        if word not in b_gram_model.keys() or next_word not in b_gram_model.loc[word].keys():\n",
    "            b_gram_prob = 0\n",
    "        else:\n",
    "            b_gram_prob = bigram_lambda *  b_gram_model.loc[word][next_word]\n",
    "\n",
    "        # unigram\n",
    "        if next_word not in unigram_model.index:\n",
    "            unigram_prob = 0\n",
    "        else:\n",
    "            unigram_prob = unigram_lambda * unigram_model.loc[next_word][0].astype(float)\n",
    "\n",
    "        prob += log_probability(unigram_prob + b_gram_prob)\n",
    "    return prob\n",
    "\n",
    "def predict_perplexity3(b_gram_model, nlp):\n",
    "    first_prob = predict_sentence_probabilty_b_gram(FIRST_SENTENCE, b_gram_model, nlp)\n",
    "    second_prob = predict_sentence_probabilty_b_gram(\n",
    "        SECOND_SENTENCE, b_gram_model, nlp)\n",
    "    first_prob = log_probability(first_prob)\n",
    "    second_prob = log_probability(second_prob)\n",
    "    held_out_data = (first_prob + second_prob) / 2\n",
    "    perplexity = 2 ** -held_out_data\n",
    "    return perplexity\n",
    "\n",
    "def predict_perplexity4(b_gram_model,unigram_model, nlp):\n",
    "    held_out_data = (linear_interpolation(FIRST_SENTENCE, unigram_model, b_gram_model, nlp) + linear_interpolation(\n",
    "    SECOND_SENTENCE, unigram_model, b_gram_model, nlp)) / (len(FIRST_SENTENCE) + len(SECOND_SENTENCE) -2) #-2 because we added START to each sentence\n",
    "    perplexity = math.exp(-held_out_data)\n",
    "    return perplexity\n",
    "\n",
    "################################ Helpers ######################################\n",
    "def convert_to_lower_case(sentence):\n",
    "    return np.char.lower(np.array(sentence))\n",
    "\n",
    "\n",
    "def predict_sentence_probabilty_unigram(sentence, unigram_model, nlp):\n",
    "    prob = 1\n",
    "    sentence = nlp(str(convert_to_lower_case(sentence)))\n",
    "\n",
    "    sentence = [token.lemma_ for token in sentence if token.is_alpha]\n",
    "    for word in sentence:\n",
    "        if word not in unigram_model.index:\n",
    "            return -math.inf\n",
    "        temp = unigram_model.loc[word][0]\n",
    "        prob *= temp\n",
    "    return prob\n",
    "\n",
    "\n",
    "def log_probability(prob):\n",
    "    if prob == 0:\n",
    "        return -math.inf\n",
    "    else:\n",
    "        return math.log(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/tamuz/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "text, nlp = load_data()\n",
    "docs, words = procces_data(text, nlp)\n",
    "\n",
    "# TASK 1\n",
    "unigram_model = train_unigram_model(docs)\n",
    "b_gram_model = train_bigram_model(docs, words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TASK 2 ###\n",
      "Predicted word: the\n",
      "\n",
      "### TASK 3 ###\n",
      "Probability of first sentence: -inf\n",
      "Probability of second sentence: -29.707484205602142\n",
      "Perplexity of both sentences: inf\n",
      "\n",
      "inf\n",
      "### TASK 4 ###\n",
      "Linear interpolation smoothing for first sentence: -36.20200520715635\n",
      "Linear interpolation smoothing for second sentence: -30.99123945115062\n",
      "Perplexity of both sentences: 270.2742150360732\n"
     ]
    }
   ],
   "source": [
    "# TASK 2\n",
    "predicted_word = continue_sentence(b_gram_model)\n",
    "print(\"### TASK 2 ###\")\n",
    "print(\"Predicted word: \" + predicted_word + \"\\n\")\n",
    "\n",
    "# TASK 3\n",
    "print(\"### TASK 3 ###\")\n",
    "#3A\n",
    "prob3a1 = predict_sentence_probabilty_b_gram(FIRST_SENTENCE, b_gram_model, nlp)\n",
    "prob3a1 = log_probability(prob3a1)\n",
    "prob3a2 = predict_sentence_probabilty_b_gram(SECOND_SENTENCE, b_gram_model, nlp)\n",
    "prob3a2 = log_probability(prob3a2)\n",
    "print(\"Probability of first sentence: \" + str(prob3a1))\n",
    "print(\"Probability of second sentence: \" + str(prob3a2))\n",
    "#3B\n",
    "perplexity3B = predict_perplexity3(b_gram_model, nlp)\n",
    "print(\"Perplexity of both sentences: \" + str(perplexity3B) + \"\\n\")\n",
    "\n",
    "print(perplexity3B)\n",
    "# TASK 4\n",
    "print(\"### TASK 4 ###\")\n",
    "prob4A = linear_interpolation(FIRST_SENTENCE, unigram_model, b_gram_model, nlp)\n",
    "prob4B = linear_interpolation(SECOND_SENTENCE, unigram_model, b_gram_model, nlp)\n",
    "perplexity4 = predict_perplexity4(b_gram_model,unigram_model, nlp)\n",
    "print(\"Linear interpolation smoothing for first sentence: \" + str(prob4A))\n",
    "print(\"Linear interpolation smoothing for second sentence: \" + str(prob4B))\n",
    "print(\"Perplexity of both sentences: \" + str(perplexity4))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
